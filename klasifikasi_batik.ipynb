{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZMWKSVt_QxJ"
      },
      "source": [
        "# Proyek Klasifikasi Gambar Capstone: Batiku\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKADPWcFKlj3"
      },
      "source": [
        "## Import Semua Packages/Library yang Digunakan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BlmvjLY9M4Yj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import zipfile\n",
        "import os\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from google.colab import drive\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3YIEnAFKrKL"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Unzip dataset\n",
        "dataset_path = \"/content/drive/MyDrive/mesin/clasification_dataset/batik.zip\""
      ],
      "metadata": {
        "id": "m44tDGbkn2H8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ekstrak file zip\n",
        "with zipfile.ZipFile(dataset_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"/content/dataset\")\n",
        "\n",
        "# Pastikan dataset berhasil diekstrak\n",
        "print(\"Ekstraksi selesai. File tersedia di /content/dataset\")"
      ],
      "metadata": {
        "id": "IamdoKwvny9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Melihat isi folder dataset\n",
        "dataset_dir = \"/content/dataset\"\n",
        "print(\"Folder dalam dataset:\", os.listdir(dataset_dir))"
      ],
      "metadata": {
        "id": "y6mUVZ50n5CK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total = sum(len(os.listdir(os.path.join(dataset_dir, label))) for label in os.listdir(dataset_dir))\n",
        "print(f\"\\nTotal dataset: {total}\")"
      ],
      "metadata": {
        "id": "GvHxng6Yn7k6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkPem5eWL2UP"
      },
      "source": [
        "### Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtpseior_ocW"
      },
      "source": [
        "#### Split Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OubAW-7ONKVj"
      },
      "outputs": [],
      "source": [
        "train_dir = \"/content/dataset_split/train\"\n",
        "val_dir = \"/content/dataset_split/val\"\n",
        "test_dir = \"/content/dataset_split/test\"\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import shutil\n",
        "\n",
        "def split_dataset(source_dir, train_dir, val_dir, test_dir, split_ratio=(0.7, 0.15, 0.15)):\n",
        "    labels = os.listdir(source_dir)\n",
        "    for label in labels:\n",
        "        img_paths = [os.path.join(source_dir, label, fname) for fname in os.listdir(os.path.join(source_dir, label))]\n",
        "        train_files, temp_files = train_test_split(img_paths, train_size=split_ratio[0], shuffle=True)\n",
        "        val_files, test_files = train_test_split(temp_files, test_size=split_ratio[2]/(split_ratio[1]+split_ratio[2]))\n",
        "\n",
        "        for path_set, target_dir in zip([train_files, val_files, test_files], [train_dir, val_dir, test_dir]):\n",
        "            label_dir = os.path.join(target_dir, label)\n",
        "            os.makedirs(label_dir, exist_ok=True)\n",
        "            for file in path_set:\n",
        "                shutil.copy(file, label_dir)\n",
        "\n",
        "# Jalankan fungsi split\n",
        "split_dataset(\"/content/dataset\", train_dir, val_dir, test_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDJuJO0jGiGp"
      },
      "source": [
        "# Preprocessing dan Augmentasi Gambar"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img_size = (150, 150)\n",
        "batch_size = 32\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, rotation_range=20, zoom_range=0.2, shear_range=0.2, horizontal_flip=True)\n",
        "val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(train_dir, target_size=img_size, batch_size=batch_size, class_mode='categorical')\n",
        "val_generator = val_test_datagen.flow_from_directory(val_dir, target_size=img_size, batch_size=batch_size, class_mode='categorical')\n",
        "test_generator = val_test_datagen.flow_from_directory(test_dir, target_size=img_size, batch_size=batch_size, class_mode='categorical', shuffle=False)\n"
      ],
      "metadata": {
        "id": "jSfd8xKdoFrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVPbB03CMhTT"
      },
      "source": [
        "## Modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnDwAMK6yqBj"
      },
      "source": [
        "# CNN"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150,150,3)),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(len(train_generator.class_indices), activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "wcwPZD0OoLTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akdpaNuzNirO"
      },
      "source": [
        "# LATIH MODEL"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=50\n",
        ")\n",
        "\n",
        "# Evaluasi pada test set\n",
        "test_loss, test_acc = model.evaluate(test_generator)\n",
        "print(\"Test Accuracy:\", test_acc)"
      ],
      "metadata": {
        "id": "U0CQqFvDoNjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ergzChZFEL-O"
      },
      "source": [
        "## Evaluasi dan Visualisasi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot akurasi dan loss selama training\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# Akurasi\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy Graph')\n",
        "\n",
        "# Loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss Graph')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FdfR0RojoPh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXHAXC361QyG"
      },
      "source": [
        "# CONVOLUTION MATRIX"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix & Classification Report\n",
        "y_pred = model.predict(test_generator)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true = test_generator.classes\n",
        "labels = list(train_generator.class_indices.keys())\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_true, y_pred_classes, target_names=labels))\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_true, y_pred_classes))"
      ],
      "metadata": {
        "id": "WlDjKE3_oR6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJEc8wTY28x4"
      },
      "source": [
        "## Konversi Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflowjs==4.12.0"
      ],
      "metadata": {
        "id": "JezItl0woUYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflowjs as tfjs\n",
        "\n",
        "# Membuat folder tujuan di Google Drive\n",
        "os.makedirs(\"/content/drive/MyDrive/mesin/tfjs_model\", exist_ok=True)\n",
        "os.makedirs(\"/content/drive/MyDrive/mesin/tflite\", exist_ok=True)\n",
        "os.makedirs(\"/content/drive/MyDrive/mesin/saved_model\", exist_ok=True)\n",
        "\n",
        "# 1. Simpan model untuk TensorFlow.js\n",
        "tfjs.converters.save_keras_model(model, \"/content/drive/MyDrive/mesin/tfjs_model\")\n",
        "\n",
        "# 2. Simpan model untuk TensorFlow Lite\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "with open(\"/content/drive/MyDrive/mesin/tflite/model.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "# Simpan label\n",
        "labels = list(train_generator.class_indices.keys())  # pastikan daftar label tersedia\n",
        "with open(\"/content/drive/MyDrive/mesin/tflite/label.txt\", \"w\") as f:\n",
        "    f.write(\"\\n\".join(labels))"
      ],
      "metadata": {
        "id": "EAx99TBGoW0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.export(\"/content/drive/MyDrive/mesin/saved_model\")"
      ],
      "metadata": {
        "id": "vV0P2FHwoeN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQTV8JtX4w8N"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "\n",
        "\n",
        "# Class labels (should match your training data)\n",
        "class_labels = []\n",
        "\n",
        "def preprocess_image(img_path, target_size=(150, 150)):\n",
        "    \"\"\"Load and preprocess an image for prediction\"\"\"\n",
        "    img = image.load_img(img_path, target_size=target_size)\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
        "    img_array /= 255.0  # Normalize like during training\n",
        "    return img_array\n",
        "\n",
        "def predict_disease(img_path, model, class_labels):\n",
        "    \"\"\"Make a prediction on a single image\"\"\"\n",
        "    # Preprocess the image\n",
        "    processed_img = preprocess_image(img_path)\n",
        "\n",
        "    # Make prediction\n",
        "    predictions = model.predict(processed_img)\n",
        "    predicted_class_index = np.argmax(predictions[0])\n",
        "    confidence = np.max(predictions[0])\n",
        "    predicted_class = class_labels[predicted_class_index]\n",
        "\n",
        "    # Display results\n",
        "    img = image.load_img(img_path)\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"Predicted: {predicted_class}\\nConfidence: {confidence:.2%}\")\n",
        "    plt.show()\n",
        "\n",
        "    # Print detailed probabilities\n",
        "    print(\"\\nPrediction probabilities:\")\n",
        "    for i, prob in enumerate(predictions[0]):\n",
        "        print(f\"{class_labels[i]}: {prob:.4f}\")\n",
        "\n",
        "    return predicted_class, confidence\n",
        "\n",
        "image_path = \"\"\n",
        "predicted_class, confidence = predict_disease(image_path, model, class_labels)"
      ],
      "metadata": {
        "id": "nIzc08D7owNf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}